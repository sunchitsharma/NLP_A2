{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing corpus and reading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL DATASET \n",
    "\n",
    "from nltk.corpus import brown\n",
    "train_lab = brown.tagged_sents()[0:len(brown.tagged_sents())*80/100]\n",
    "test = brown.sents()[0:len(brown.sents())*20/100][11:12]\n",
    "test_lab=brown.tagged_sents()[0:len(brown.tagged_sents())*20/100][11:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(u'BEM*',): 4, (u'HVN',): 185, (u'WP$',): 230, (u'HV*',): 16, (u'(',): 2341, (u'AP',): 8408, (u'PP$$',): 110, (u'PPLS',): 314, (u'PPL',): 913, (u'RN',): 7, (u'EX',): 1887, (u'DO*',): 292, (u'PN',): 1820, (u'PPS',): 13571, (u'NN$',): 1569, (u'BEDZ*',): 70, (u'PPSS',): 10485, (u'CD',): 13802, (u'',): 2782, (u'DTI',): 2581, (u')',): 2372, (u'VBG',): 14731, (u'DTX',): 85, (u'DT$',): 4, (u'.',): 48424, (u'CC',): 32177, (u'NPS',): 1215, (u'NP$',): 2253, (u'NN',): 146799, (u'TO',): 12732, (u'VBN',): 26374, (u'NR$',): 75, (u'CS',): 18871, (u'MD*',): 556, (u'BE',): 5724, (u'VB',): 28017, (u'NNS$',): 296, (u'WRB',): 3641, (u'DOZ*',): 63, (u\"''\",): 6119, (u'NP',): 33668, (u'WPS',): 3469, (u'NNS',): 52089, (u'QL',): 7547, (u'HVD*',): 46, (u'RBT',): 88, (u'NIL',): 157, (u'HV',): 3400, (u'HVZ',): 2360, (u'DOD',): 759, ('<s>',): 45872, (u'BEN',): 2058, (u'JJT',): 903, (u'RB$',): 8, (u'MD',): 10767, (u'RP',): 4289, (u'BER*',): 30, (u\"'\",): 250, (u'ABL',): 313, (u'BEG',): 593, (u'DT',): 7756, (u'WQL',): 180, (u'DO',): 1071, (u'``',): 6160, (u'HVD',): 3327, (u'DOZ',): 456, (u'JJS',): 355, (u'VBD',): 17930, (u',',): 48494, (u'QLP',): 190, (u'WDT',): 4845, (u'NR',): 1651, (u'AT',): 85722, (u'BEZ*',): 62, (u'BED*',): 8, (u'HVZ*',): 14, (u'FW',): 1047, (u'RBR',): 950, (u'BED',): 2659, (u'VBZ',): 7092, (u'CD$',): 5, (u'RB',): 29426, (u'PPO',): 7505, (u'JJR',): 1763, (u'NRS',): 15, (u'BEDZ',): 7334, (u':',): 1540, (u'NPS$',): 36, (u'BER',): 4191, (u'WPO',): 271, (u'ABN',): 2482, (u'DTS',): 2219, (u'AP$',): 6, (u'BEZ',): 9671, (u'*',): 3935, (u'BEM',): 181, (u'HVG',): 224, (u'JJ',): 60483, (u'IN',): 107461, (u'DOD*',): 194, (u'ABX',): 647, (u'UH',): 318, (u'JJ$',): 1, (u'PN$',): 81, (u'OD',): 1954, (u'PP$',): 13025}\n"
     ]
    }
   ],
   "source": [
    "from itertools import tee, izip\n",
    "\n",
    "transition_tri={}\n",
    "transition_bi={}\n",
    "context={}\n",
    "emission={}\n",
    "\n",
    "def window(iterable, size):\n",
    "    iters = tee(iterable, size)\n",
    "    for i in xrange(1, size):\n",
    "        for each in iters[i:]:\n",
    "            next(each, None)\n",
    "    return izip(*iters)\n",
    "\n",
    "\n",
    "# MAKING DICT {(TAG,TAG,TAG) : COUNT}\n",
    "           \n",
    "\n",
    "for i in train_lab:\n",
    "    i=[(\"START\",\"<s>\")]+i\n",
    "    i=i+[(\"END\",\"</s>\")]\n",
    "    for each in window(i,3):\n",
    "        temp=list(each)\n",
    "        temp2=[]\n",
    "        temp2.append(temp[0][1])\n",
    "        temp2.append(temp[1][1])\n",
    "        temp2.append(temp[2][1])\n",
    "        temp=temp2\n",
    "        for i in range(0,len(temp)):\n",
    "            if \"-\" in temp[i]:\n",
    "                temp[i]=temp[i].split(\"-\")[0]\n",
    "            if \"+\" in temp[i]:\n",
    "                temp[i]=temp[i].split(\"+\")[0]\n",
    "        if transition_tri.has_key(tuple(temp)):\n",
    "            transition_tri[tuple(temp)]+=1\n",
    "        else:\n",
    "            transition_tri[tuple(temp)]=1\n",
    "\n",
    "\n",
    "            \n",
    "# MAKING DICT {(TAG,TAG) : COUNT}\n",
    "\n",
    "\n",
    "for i in train_lab:\n",
    "    i=[(\"START\",\"<s>\")]+i\n",
    "    i=i+[(\"END\",\"</s>\")]\n",
    "    for each in window(i,2):\n",
    "        temp=list(each)\n",
    "        temp2=[]\n",
    "        temp2.append(temp[0][1])\n",
    "        temp2.append(temp[1][1])\n",
    "        temp=temp2\n",
    "        for i in range(0,len(temp)):\n",
    "            if \"-\" in temp[i]:\n",
    "                temp[i]=temp[i].split(\"-\")[0]\n",
    "            if \"+\" in temp[i]:\n",
    "                temp[i]=temp[i].split(\"+\")[0]\n",
    "        if transition_bi.has_key(tuple(temp)):\n",
    "            transition_bi[tuple(temp)]+=1\n",
    "        else:\n",
    "            transition_bi[tuple(temp)]=1\n",
    "            \n",
    "\n",
    "            \n",
    "# MAKING CONTEXT {(TAG) : COUNT}\n",
    "\n",
    "\n",
    "for i in train_lab:\n",
    "    i=[(\"START\",\"<s>\")]+i\n",
    "    for each in window(i,1):\n",
    "        temp=list(each)\n",
    "        temp2=[]\n",
    "        temp2.append(temp[0][1])\n",
    "        temp=temp2\n",
    "        for i in range(0,len(temp)):\n",
    "            if \"-\" in temp[i]:\n",
    "                temp[i]=temp[i].split(\"-\")[0]\n",
    "            if \"+\" in temp[i]:\n",
    "                temp[i]=temp[i].split(\"+\")[0]\n",
    "        if context.has_key(tuple(temp)):\n",
    "            context[tuple(temp)]+=1\n",
    "        else:\n",
    "            context[tuple(temp)]=1\n",
    "\n",
    "\n",
    "# MAKING EMISSION { (TAG,WORD) : COUNT }\n",
    "\n",
    "N=0\n",
    "\n",
    "for i in train_lab:\n",
    "    i=[(\"START\",\"<s>\")]+i\n",
    "    i=i+[(\"END\",\"</s>\")]\n",
    "    for each in window(i,1):\n",
    "        N+=1\n",
    "        temp=list(each)\n",
    "        temp2=[]\n",
    "        temp2.append(temp[0][1])\n",
    "        temp2.append((temp[0][0]).lower())\n",
    "        temp=temp2\n",
    "        for i in range(0,len(temp)):\n",
    "            if \"-\" in temp[i]:\n",
    "                temp[i]=temp[i].split(\"-\")[0]\n",
    "            if \"+\" in temp[i]:\n",
    "                temp[i]=temp[i].split(\"+\")[0]\n",
    "        if emission.has_key(tuple(temp)):\n",
    "            emission[tuple(temp)]+=1\n",
    "        else:\n",
    "            emission[tuple(temp)]=1\n",
    "        \n",
    "       \n",
    "print context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation and Transition Probability Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_1 = 0.90\n",
    "lam_2 = 0.90\n",
    "lam_3 = 0.90\n",
    "\n",
    "def Transition_Prob(trig):\n",
    "    if len(trig) > 2:                      # IT IS A TRIGRAM\n",
    "        \n",
    "        prev1 = trig[0]\n",
    "        prev2 = trig[1]\n",
    "        tag = trig[2]\n",
    "        \n",
    "        P1 = lam_1 * context[tuple([prev1])]/N + (1-lam_1)/N\n",
    "        \n",
    "        P2 = lam_2 * transition_bi.get(tuple((prev1+\" \"+prev2).split()),0)/(context[tuple([prev1])]+1)+(1-lam_2)*P1\n",
    "        \n",
    "        P3 = lam_3*transition_tri.get(tuple((prev1+\" \"+prev2+\" \"+tag).split()),0)/(transition_bi.get(tuple((prev1+\" \"+prev2).split()),0)+1)+(1-lam_3)*P2\n",
    "        return P3\n",
    "    else:                                            # ITS A BIGRAM\n",
    "        prev = trig[0]\n",
    "        tag  = trig[1]\n",
    "        \n",
    "        P1 = lam_1 * context[tuple([prev])]/N + (1-lam_1)/N\n",
    "        \n",
    "        P2 = lam_2 * transition_bi.get(tuple((prev+\" \"+tag).split()),0)/(context[tuple([prev])]+1)+(1-lam_2)*P1\n",
    "        return P2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Emission Probability with some smoothing probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0.90\n",
    "\n",
    "def Emission_Prob(tag2word):\n",
    "    \n",
    "    tag = tag2word[0]\n",
    "    word = tag2word[1]\n",
    "    \n",
    "    prob = lamda * emission.get(tag2word,0)/context[tuple([tag])]+(1-lamda)/N\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def Fetch_Context(k):\n",
    "    if k == -1 or k == 0:               # In case we are at beginning retun start\n",
    "        return [tuple([\"<s>\"])];\n",
    "    else:                               # For a word return full sentence\n",
    "        return context\n",
    "\n",
    "\n",
    "def forward_step(line):\n",
    "    \n",
    "    words=[]\n",
    "    \n",
    "    for i in line:\n",
    "        words.append(i.lower())\n",
    "    \n",
    "    # NOW WE HAVE A LIST OF WORDS FOR A SENTENCE, ALL IN LOWER CASE\n",
    "    \n",
    "    l=len(words)\n",
    "    \n",
    "    best_score={}\n",
    "    best_edge={}\n",
    "    \n",
    "    best_score[(0,\"<s>\",\"<s>\")]=0     # init\n",
    "    best_edge[(0,\"<s>\",\"<s>\")]=None   # init\n",
    "    \n",
    "    for k in range(1,l+1):        # Travel in a sentence  \n",
    "        \n",
    "        for u in Fetch_Context(k-1):\n",
    "            for v in Fetch_Context(k):\n",
    "                for w in Fetch_Context(k-2):\n",
    "                    \n",
    "                    tri=tuple([w[0],u[0],v[0]])\n",
    "                    if emission.get(tuple((v[0]+\" \"+words[k-1]).split()),0)!=0:\n",
    "                        \n",
    "                        score=best_score.get(tuple([k-1,w[0],u[0]]),0)+(-math.log(Transition_Prob(tri)))+(-math.log(Emission_Prob(tuple([v[0],words[k-1]]))))\n",
    "                        \n",
    "                        if best_score.get(tuple([k,u[0],v[0]]),0) == 0 or best_score.get(tuple([k,u[0],v[0]]),0) < score:\n",
    "                            \n",
    "                            best_score[tuple([k,u[0],v[0]])]=score\n",
    "                            best_edge[tuple([k,u[0],v[0]])]=w[0]\n",
    "\n",
    "    \n",
    "    # NOW FOR \n",
    "    \n",
    "    max_score = float('-Inf')\n",
    "    \n",
    "    u_max = None\n",
    "    v_max = None\n",
    "    \n",
    "    for u in S(l-1):\n",
    "        for v in S(l):\n",
    "            \n",
    "            tri=tuple([u[0],v[0],\"</s>\"])\n",
    "            \n",
    "            score=best_score.get(tuple([l,u[0],v[0]]),0)+(-math.log(Transition_Prob(tri)))\n",
    "            \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                u_max = u[0]\n",
    "                v_max = v[0]\n",
    "    \n",
    "    tags = []\n",
    "    tags.append(v_max)\n",
    "    tags.append(u_max)\n",
    "    \n",
    "    for i, k in enumerate(range(l-2, 0, -1)):\n",
    "        tags.append(best_edge.get(tuple([k+2,tags[i+1],tags[i]]),\"NN\"))\n",
    "    tags.reverse()\n",
    "\n",
    "    tagged_sentence = []\n",
    "    for j in range(0, l):\n",
    "        tagged_sentence.append((words[j],tags[j]))\n",
    "    \n",
    "    return tagged_sentence\n",
    "      \n",
    "\n",
    "def calcacc(dataset,lab_dataset):\n",
    "    total=0\n",
    "    for i in dataset:\n",
    "        total+=len(i)\n",
    "    \n",
    "    correct =0\n",
    "    \n",
    "    for i in range(0,len(dataset)):\n",
    "        x=forward_step(dataset[i])\n",
    "        for j in range(0,len(x)):\n",
    "            if x[j]==lab_dataset[i][j]:\n",
    "                correct +=1\n",
    "    \n",
    "    print (correct*1.0/total)*100\n",
    "\n",
    "\n",
    "calcacc(test,test_lab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
